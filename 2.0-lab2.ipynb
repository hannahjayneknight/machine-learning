{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand Backpropagation\n",
    "2. Write a neural network with one or more hidden layers\n",
    "3. Solve the XOR\n",
    "4. Understand how to build general classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all relevant code from the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    M = 1/(1+np.exp(-(X.dot(W.T)+B)))\n",
    "    return M\n",
    "\n",
    "def diff_W(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B)) # differentiating sigm function\n",
    "    dW = (Y-Z)*dS\n",
    "\n",
    "    return X.T.dot(dW) # dot product between X transpose and dW\n",
    "\n",
    "def diff_B(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B))\n",
    "    dB = (Y-Z)*dS\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "X = np.random.randint(2, size=[15, 2]) # produces an array size [15, 2] containing either 0 or 1\n",
    "Z = np.array( [X[:,0] ^ X[:,1] ]).T\n",
    "\n",
    "X_Test = np.random.randint(2, size=[15, 2])\n",
    "Y_Test = np.array(X[:,0] ^ X[:,1] ).T\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\i\\lab2.png\" width=\"400\"> </br>\n",
    "**Why have the dimensions of the weights and biases changed from lab 1?** </br>\n",
    "The structure in the image above shows how for this lab, there are 2 inputs (1 or 0), 2 layers (first layer has three sigmoids, second layer has one sigmoid) and one output. </br> </br>\n",
    "\n",
    "W1: 3 sets of weights for the 3 sigmoids in layer 1. 2 weights in each set corresponding to the two inputs. </br>\n",
    "B1: 3 biases for the 3 sigmoids in layer 1.</br></br>\n",
    "\n",
    "W2: The problem has been reduced to one sigmoid therefore there is one set of weights. This one set contains 3 terms to account for the 3 outputs from the previous layer.\n",
    "B2: 1 bias for the 1 sigmoid in layer 2.\n",
    "\n",
    "</br></br>\n",
    "**Why do we need 3 sigmoids in layer 1?** </br>\n",
    "We don't. 3 was a randomly chosen number. We need at least 2. You can try with 2 and should see the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in a forward function to reflect the network topology that we want to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(X, W1, B1, W2, B2):\n",
    "    #first layer\n",
    "\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    #second layer\n",
    "\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # We return both the final output and the output from the hidden layer\n",
    "\n",
    "    return Y, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of backpropogation functions \n",
    "\n",
    "// insert derivation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_B2(Z, Y):\n",
    "    dB = (Z-Y)*Y*(1-Y)\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z-Y)*Y*(1-Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous lab, we are not making use of the sigmoid function inside the update rules. Instead, we feed them the outputs from the middle layer (H, in this example). The results are the same and which expression you use is simply a matter of readbility and compactness of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Accuracy:  0.5038360735248066\n",
      "Epoch:  50  Accuracy:  0.7594914978953156\n",
      "Epoch:  100  Accuracy:  0.7699804088500772\n",
      "Epoch:  150  Accuracy:  0.7840129363956222\n",
      "Epoch:  200  Accuracy:  0.8045644446560197\n",
      "Epoch:  250  Accuracy:  0.8334493491672781\n",
      "Epoch:  300  Accuracy:  0.8681677922567888\n",
      "Epoch:  350  Accuracy:  0.9016801000017931\n",
      "Epoch:  400  Accuracy:  0.9285187679319801\n",
      "Epoch:  450  Accuracy:  0.9476763961131898\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(500):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 = W2 + learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 = B2 + learning_rate * diff_B2(Z, Y)\n",
    "    W1 = W1 + learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 = B1 + learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 -np.mean((Z-Y)**2)\n",
    "        print(\"Epoch: \", epoch, \" Accuracy: \", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax turns the numeric output of the last linear layer of a multi-class classification neural network into a probability with the probabilities summing to 1.\n",
    "\n",
    "The exponent is taken for each output then normalized by dividing by the sum of all the exponents. Softmax function:\n",
    "<img src=\".\\i\\softmax.png\" width=\"400\"> </br>\n",
    "\n",
    "Softmax is an activation function. Given it is a multivariable function, we cannot plot it like the sigmoid.\n",
    "\n",
    "**From lab notes:** This representation is ideal when your label z, is a “one-hot vector”, where there is 1 one (the correct class) and N-1 zeros (the incorrect ones). For instance, imagine that you were trying to teach a network to distinguish between images of cats and dogs. \n",
    "\n",
    "Softmax is usually used on the last layer of an image classification network.\n",
    "\n",
    "Source: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267],\n",
       "       [0.92870733, 0.07129267]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing softmax\n",
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(1, 2)\n",
    "B1 = np.random.randn(2)\n",
    "X = np.random.randint(2, size=[15, 2])\n",
    "\n",
    "def softmax(X, W, B): # my attempt\n",
    "    exps = [np.exp(i) for i in X.dot(W.T)+B]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    return softmax\n",
    "\n",
    "def softmax2(X, W, B): # correct solution\n",
    "    A = X.dot(W.T) + B\n",
    "    expA = np.exp(A)\n",
    "    output = expA/expA.sum(axis=1, keepdims = True)\n",
    "    return output\n",
    "\n",
    "softmax2(X, W1, B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why my attempt was wrong:\n",
    "- When summing the exponentials, my function summed all the column elements (rather than the rows) meaning I got 2 outputs instead of 15.\n",
    "\n",
    "Note that in the returned array:\n",
    "- Each row gives the probability of each class being the correct output.\n",
    "- The sum of each row is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy function\n",
    "\n",
    "This is usually used as the loss function alongside Softmax.\n",
    "\n",
    "<img src=\".\\i\\crossentropy.png\" width=\"400\"> </br>\n",
    "\n",
    "**From lab notes:** Cross-entropy loss, measures the performance of a classification model whose output is a probability value between 0 and 1, and the error increases as the predicted probability deviates from the actual label. A perfect model would have a loss of 0.\n",
    "\n",
    "**Example:** the expected output is [0, 1] but the output from the neural net is [0.05, 0.95].\n",
    "\n",
    "<img src=\".\\i\\crossentropy2.png\" width=\"400\"> </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating XOR to use Softmax and Cross-entropy\n",
    "\n",
    "The following code is the same as above with the following changes:\n",
    "- The sigmoid function has been replaced with the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n",
    "\n",
    "def softmax(X, W, B):\n",
    "    exps = [np.exp(i) for i in X.dot(W.T)+B]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    return softmax\n",
    "\n",
    "def diff_W(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B)) # differentiating sigm function\n",
    "    dW = (Y-Z)*dS\n",
    "\n",
    "    return X.T.dot(dW) # dot product between X transpose and dW\n",
    "\n",
    "def diff_B(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B))\n",
    "    dB = (Y-Z)*dS\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "X = np.random.randint(2, size=[15, 2]) # produces an array size [15, 2] containing either 0 or 1\n",
    "Z = np.array( [X[:,0] ^ X[:,1] ]).T\n",
    "\n",
    "X_Test = np.random.randint(2, size=[15, 2])\n",
    "Y_Test = np.array(X[:,0] ^ X[:,1] ).T\n",
    "\n",
    "def Forward(X, W1, B1, W2, B2):\n",
    "    #first layer\n",
    "\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    #second layer\n",
    "\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # We return both the final output and the output from the hidden layer\n",
    "\n",
    "    return Y, H\n",
    "\n",
    "def diff_B2(Z, Y):\n",
    "    dB = (Z-Y)*Y*(1-Y)\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z-Y)*Y*(1-Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)).sum(axis=0)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(500):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 = W2 + learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 = B2 + learning_rate * diff_B2(Z, Y)\n",
    "    W1 = W1 + learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 = B1 + learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 -np.mean((Z-Y)**2)\n",
    "        print(\"Epoch: \", epoch, \" Accuracy: \", Accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8944ac20941febd3898b0b9a08c0e7cafc7c4835a26d6ed6b5e365f07f1b2728"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
