{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand Backpropagation\n",
    "2. Write a neural network with one or more hidden layers\n",
    "3. Solve the XOR\n",
    "4. Understand how to build general classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all relevant code from the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    M = 1/(1+np.exp(-(X.dot(W.T)+B)))\n",
    "    return M\n",
    "\n",
    "def diff_W(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B)) # differentiating sigm function\n",
    "    dW = (Y-Z)*dS\n",
    "\n",
    "    return X.T.dot(dW) # dot product between X transpose and dW\n",
    "\n",
    "def diff_B(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B))\n",
    "    dB = (Y-Z)*dS\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "X = np.random.randint(2, size=[15, 2]) # produces an array size [15, 2] containing either 0 or 1\n",
    "Z = np.array( [X[:,0] ^ X[:,1] ]).T\n",
    "\n",
    "X_Test = np.random.randint(2, size=[15, 2])\n",
    "Y_Test = np.array(X[:,0] ^ X[:,1] ).T\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# for epoch in range(500):\n",
    "#     output = sigm(X, W, B)\n",
    "\n",
    "#     W = W + (learning_rate * diff_W(X, output, Y, B, W).T)\n",
    "#     B = B + learning_rate * diff_B(X, output, Y, B, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\i\\lab2.png\" width=\"400\"> </br>\n",
    "**Why have the dimensions of the weights and biases changed from lab 1?** </br>\n",
    "The structure in the image above shows how for this lab, there are 2 inputs (1 or 0), 2 layers (first layer has three sigmoids, second layer has one sigmoid) and one output. </br> </br>\n",
    "\n",
    "W1: 3 sets of weights for the 3 sigmoids in layer 1. 2 weights in each set corresponding to the two inputs. </br>\n",
    "B1: 3 biases for the 3 sigmoids in layer 1.</br></br>\n",
    "\n",
    "W2: The problem has been reduced to one sigmoid therefore there is one set of weights. This one set contains 3 terms to account for the 3 outputs from the previous layer.\n",
    "B2: 1 bias for the 1 sigmoid in layer 2.\n",
    "\n",
    "</br></br>\n",
    "**Why do we need 3 sigmoids in layer 1?** </br>\n",
    "We don't. 3 was a randomly chosen number. We need at least 2. You can try with 2 and should see the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in a forward function to reflect the network topology that we want to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(X, W1, B1, W2, B2):\n",
    "    #first layer\n",
    "\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    #second layer\n",
    "\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # We return both the final output and the output from the hidden layer\n",
    "\n",
    "    return Y, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of backpropogation functions \n",
    "\n",
    "// insert derivation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_B2(Z, Y):\n",
    "    dB = (Z-Y)*Y*(1-Y)\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z-Y)*Y*(1-Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous lab, we are not making use of the sigmoid function inside the update rules. Instead, we feed them the outputs from the middle layer (H, in this example). The results are the same and which expression you use is simply a matter of readbility and compactness of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Accuracy:  0.5038360735248066\n",
      "Epoch:  50  Accuracy:  0.7594914978953156\n",
      "Epoch:  100  Accuracy:  0.7699804088500772\n",
      "Epoch:  150  Accuracy:  0.7840129363956222\n",
      "Epoch:  200  Accuracy:  0.8045644446560197\n",
      "Epoch:  250  Accuracy:  0.8334493491672781\n",
      "Epoch:  300  Accuracy:  0.8681677922567888\n",
      "Epoch:  350  Accuracy:  0.9016801000017931\n",
      "Epoch:  400  Accuracy:  0.9285187679319801\n",
      "Epoch:  450  Accuracy:  0.9476763961131898\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(500):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 = W2 + learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 = B2 + learning_rate * diff_B2(Z, Y)\n",
    "    W1 = W1 + learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 = B1 + learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 -np.mean((Z-Y)**2)\n",
    "        print(\"Epoch: \", epoch, \" Accuracy: \", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General classfiers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8944ac20941febd3898b0b9a08c0e7cafc7c4835a26d6ed6b5e365f07f1b2728"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
