{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand how to use Pytorch to write a Neural Network\n",
    "2. Write a neural network with multiple hidden layers\n",
    "\n",
    "In this lab, linear layers are used. The lab is building the following network:</br>\n",
    "<img src=\".\\i\\lab3-network.png\" width=\"400\"> </br> </br>\n",
    "\n",
    "This has:\n",
    "- A 28*28 pixel input (==784 dimensional vector / a 784 1-dimensional tensor)\n",
    "- 3 hidden layers\n",
    "- Each hidden layer has 64 neurons.\n",
    "- The output layer can be an integer between 0 and 9 (==10 dimensional vector / a 10 1-dimensional tensor)\n",
    "- Sigmoid activation functions in the hidden layers\n",
    "- Softmax functions for the neurons in the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data.\n",
    "\n",
    "We are using the MNIST database: http://yann.lecun.com/exdb/mnist/.\n",
    "- The input is a 28 by 28 matrix (2-dimensional tensor), which corresponds to the\n",
    "pixels of the image.\n",
    "- The output is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\n",
    "    \"\", # Root directory of where dataset exists or will be saved to if download is set to True.\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]) # .ToTensor() converts a PIL Image or numpy.ndarray to tensor.\n",
    "    ) #compose allows you to do multiple transformations\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# DataLoader() wraps an iterable over the given dataset and supports automatic batching, sampling, shuffling and multiprocess data loading\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)\n",
    "\n",
    "# batch_size determines how many samples to pass through network before w and b are updated\n",
    "# reduces memory downloaded and increases speed to train\n",
    "# one epoch involves number of samples/ batch_size updates to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding tensor dimensions:  </br> </br>\n",
    "\n",
    "<img src=\".\\i\\tensors.png\" width=\"400\"> </br> </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # linear transformation, ùë•ùêñ+ùëèxW+b, 28*28 inputs, 64 outputs\n",
    "        self.fc2 = nn.Linear(64, 64) # automatically creates the weight and bias tensors\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # how nn.Linear works\n",
    "        # m = nn.Linear(20, 30)\n",
    "        # output = m(input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x)) # passing the linearly transformed input through a neuron\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x) # apply linear function to x before applying softmax (can be done in line below)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: ```net.zero_grad()``` \n",
    "\n",
    "This resets the gradient to 0 for each variable in the network. Otherwise, at each iteration, Pytorch's \"autograd\" feature (which allows tensors to keep track of operations that involved them and provide series of derivative wrt their inputs) will keep adding the new value of the gradient to the previous one.\n",
    "\n",
    "This is because by default, gradients are accumulated in buffers( i.e, not overwritten) whenever ```.backward()``` is called.\n",
    "\n",
    "Without this line, the method ```loss.backward()``` would accumulate over each iteration leading to an incorrect training.\n",
    "\n",
    "#### Why can't the gradients be automatically zeroed when ```loss.backward()``` is called?\n",
    "\n",
    "There are two cases when the previous gradient is needed:\n",
    "1. When we want to perform gradient descent, the process is separated into finding the gradient and performing a step. First, ```loss.backward()``` is called followed by ```optimizer.step()``` second. This is why it needs to remember the gradient.\n",
    "2. Sometimes, we need to accumulate gradient among some batches; to do that, we can simply call backward multiple times and optimize once.\n",
    "\n",
    "More info on autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the optimizer, the error function, and compute the gradient wrt the loss and then train for number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001) # optimization method in backpropogation\n",
    "\n",
    "for epoch in range(3): # number of training epochs\n",
    "    for data in trainset: # iterating over training data\n",
    "        X, y = data # assign input (X) and labels (y)\n",
    "        net.zero_grad() # see note above\n",
    "        output = net.forward( # compute the output using the forward method\n",
    "            X.view(-1, 28*28) # transform 28*28 tensor input to a 784 vector\n",
    "        )\n",
    "        loss = F.nll_loss(output, y) # cross-entropy loss function since this is a classifier\n",
    "        loss.backward() # compute gradient wrt loss function over each parameter of network\n",
    "        optimizer.step() # update parameters of the network according to optimisation algorithm and gradient stored within each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network and test.\n",
    "\n",
    "NB: ```torch.no_grad()``` means network won't update gradient stored in each variable during the test session (as there is no loss to be minimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.114\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]: # if ouput with highest probability equals the expected output\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running with ReLu functions and Adam's gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.93\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) # softmax is an activation function that we need to apply wx+b to\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.Adamax(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results for different activation functions and optimizers** </br>\n",
    "Sigmoids and SGD: accuracy == 0.098. --> to get a higher accuracy here, you need to train for many more epochs, but using SGD that will be very slow.\n",
    "\n",
    "ReLu and SGD: 0.141\n",
    "\n",
    "Sigmoids and Adam: accuracy == 0.942\n",
    "\n",
    "ReLu and Adam: accuracy == 0.939\n",
    "\n",
    "_ReLu and AdaMax: accuracy == 0.93_\n",
    "\n",
    "PLEASE SEE NOTEBOOK 3.1 FOR FUTHER DETAILS ON THIS AND CHANGING THE NUMBER OF LAYERS IN THE NETWORK.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ec054b59268ae76760b157b3eba00de255c2d102e132ab4e8bc4a94d89a299"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch_p37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
