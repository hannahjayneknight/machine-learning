{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand how to use Pytorch to write a Neural Network\n",
    "2. Write a neural network with multiple hidden layers\n",
    "\n",
    "In this lab, linear layers are used. The lab is building the following network:</br>\n",
    "<img src=\".\\i\\lab3-network.png\" width=\"400\"> </br> </br>\n",
    "\n",
    "This has:\n",
    "- A 28*28 pixel input (==784 dimensional vector / a 784 1-dimensional tensor)\n",
    "- 3 hidden layers\n",
    "- Each hidden layer has 64 neurons.\n",
    "- The output layer can be an integer between 0 and 9 (==10 dimensional vector / a 10 1-dimensional tensor)\n",
    "- Sigmoid activation functions in the hidden layers\n",
    "- Softmax functions for the neurons in the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data.\n",
    "\n",
    "We are using the MNIST database: http://yann.lecun.com/exdb/mnist/.\n",
    "- The input is a 28 by 28 matrix (2-dimensional tensor), which corresponds to the\n",
    "pixels of the image.\n",
    "- The output is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\n",
    "    \"\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.Compose([transforms.ToTensor()])) #compose allows you to do multiple transformations, .ToTensor() converts a PIL Image or numpy.ndarray to tensor.\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# DataLoader() wraps an iterable over the given dataset and supports automatic batching, sampling, shuffling and multiprocess data loading\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # linear transformation, ùë•ùêñ+ùëèxW+b, 28*28 inputs, 64 outputs\n",
    "        self.fc2 = nn.Linear(64, 64) # automatically creates the weight and bias tensors\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x)) # passing the linearly transformed input through a neuron\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the optimizer, the error function, and compute the gradient wrt the loss and then train for number of epochs.\n",
    "\n",
    "NB: ```net.zero_grad()``` resets the gradient to 0 for each variable in the network. Otherwise, at each iteration, Pytorch's \"autograd\" feature (which allows tensors to keep track of operations that involved them and provide series of derivative wrt their inputs) will keep adding the new value of the gradient to the previous one.\n",
    "\n",
    "Without this line, the method ```loss.backward()``` would accumulate over each iteration leading to an incorrect training.\n",
    "\n",
    "More info on autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001) # optimization method in backpropogation\n",
    "\n",
    "for epoch in range(3): # number of training epochs\n",
    "    for data in trainset: # iterating over training data\n",
    "        X, y = data # assign input (X) and labels (y)\n",
    "        net.zero_grad() # see note above\n",
    "        output = net.forward( # compute the output using the forward method\n",
    "            X.view(-1, 28*28) # transform 28*28 tensor input to a 784 vector\n",
    "        ) \n",
    "        loss = F.nll_loss(output, y) # cross-entropy loss function since this is a classifier\n",
    "        loss.backward() # compute gradient wrt loss function over each parameter of network\n",
    "        optimizer.step() # update parameters of the network according to optimisation algorithm and gradient stored within each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train network and test.\n",
    "\n",
    "NB: ```torch.no_grad()``` means network won't update gradient stored in each variable during the test session (as there is no loss to be minimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.942\n"
     ]
    }
   ],
   "source": [
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running with ReLu functions and Adam's gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.104\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results for different activation functions and optimizers** </br>\n",
    "Sigmoids and SGD: accuracy == 0.098. --> to get a higher accuracy here, you need to train for many more epochs, but using SGD that will be very slow.\n",
    "\n",
    "ReLu and SGD: 0.141\n",
    "\n",
    "Sigmoids and Adam: accuracy == 0.942\n",
    "\n",
    "ReLu and Adam: accuracy == 0.939\n",
    "\n",
    "PLEASE SEE NOTEBOOK 3.1 FOR FUTHER DETAILS ON THIS AND CHANGING THE NUMBER OF LAYERS IN THE NETWORK.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ec054b59268ae76760b157b3eba00de255c2d102e132ab4e8bc4a94d89a299"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch_p37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
