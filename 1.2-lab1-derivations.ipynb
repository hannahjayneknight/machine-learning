{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of functions\n",
    "\n",
    "_\"\"Derive an analytical expressionn for the error function. In this lab, the squared error function is used as the error function.\"\"_\n",
    "\n",
    "Firstly, we derive dS.\n",
    "\n",
    "<img src=\".\\i\\dS.png\" width=\"400\"> </br>\n",
    "\n",
    "Next, we can derive the output of ```diff_W``` by differentiating the error term and using the chain rule.\n",
    "\n",
    "<img src=\".\\i\\diff_W.png\" width=\"400\"> </br>\n",
    "\n",
    "Lastly, we can derive the output of ```diff_B``` by doing the same for the bias.\n",
    "\n",
    "<img src=\".\\i\\diff_B.png\" width=\"400\"> </br>\n",
    "\n",
    "**Note the shape of the outputs.** The shape of dE/dB will be the shape of y_hat (y_hat, O and sigm should have the same shape). The shape of dE/dB, however, must coincide with the shape of B. That's why there is summation over the first axis.\n",
    "\n",
    "\n",
    "Because the updates are W = W + lr * dE/dW and B = B + lr * dE/dB, you should always remember that shape of dE/dW must be the same as shape of W, and shape of dE/dB should be the same as shape of B, since lr is just a scalar.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
