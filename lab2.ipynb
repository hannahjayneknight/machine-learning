{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand Backpropagation\n",
    "2. Write a neural network with one or more hidden layers\n",
    "3. Solve the XOR\n",
    "4. Understand how to build general classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all relevant code from the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(3, 2)\n",
    "B1 = np.random.randn(3)\n",
    "W2 = np.random.randn(1, 3)\n",
    "B2 = np.random.randn(1)\n",
    "\n",
    "def sigm(X, W, B):\n",
    "    M = 1/(1+np.exp(-(X.dot(W.T)+B)))\n",
    "    return M\n",
    "\n",
    "def diff_W(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B)) # differentiating sigm function\n",
    "    dW = (Y-Z)*dS\n",
    "\n",
    "    return X.T.dot(dW) # dot product between X transpose and dW\n",
    "\n",
    "def diff_B(X, Z, Y, B, W):\n",
    "\n",
    "    dS = sigm(X, W, B)*(1-sigm(X, W, B))\n",
    "    dB = (Y-Z)*dS\n",
    "\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "X = np.random.randint(2, size=[15, 2]) # produces an array size [15, 2] containing either 0 or 1\n",
    "Y = np.array( [X[:,0] ^ X[:,1] ]).T\n",
    "\n",
    "X_Test = np.random.randint(2, size=[15, 2])\n",
    "Y_Test = np.array(X[:,0] | X[:,1] ).T\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# for epoch in range(500):\n",
    "#     output = sigm(X, W, B)\n",
    "\n",
    "#     W = W + (learning_rate * diff_W(X, output, Y, B, W).T)\n",
    "#     B = B + learning_rate * diff_B(X, output, Y, B, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\images\\lab2.png\" width=\"400\"> </br>\n",
    "**Why have the dimensions of the weights and biases changed from lab 1?** </br>\n",
    "The structure in the image above shows how for this lab, there are 2 inputs (1 or 0), 2 layers (first layer has three sigmoids, second layer has one sigmoid) and one output. </br> </br>\n",
    "\n",
    "W1: 3 sets of weights for the 3 sigmoids in layer 1. 2 weights in each set corresponding to the two inputs. </br>\n",
    "B1: 3 biases for the 3 sigmoids in layer 1.</br></br>\n",
    "\n",
    "W2: The problem has been reduced to one sigmoid therefore there is one set of weights. This one set contains 3 terms to account for the 3 outputs from the previous layer.\n",
    "B2: 1 bias for the 1 sigmoid in layer 2.\n",
    "\n",
    "</br></br>\n",
    "**Why do we need 3 sigmoids in layer 1?** </br>\n",
    "We don't. 3 was a randomly chosen number. We need at least 2. You can try with 2 and should see the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in a forward function to reflect the network topology that we want to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(X, W1, B1, W2, B2):\n",
    "    #first layer\n",
    "\n",
    "    H = sigm(X, W1, B1)\n",
    "\n",
    "    #second layer\n",
    "\n",
    "    Y = sigm(H, W2, B2)\n",
    "\n",
    "    # We return both the final output and the output from the hidden layer\n",
    "\n",
    "    return Y, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of backpropogation functions \n",
    "\n",
    "// insert derivation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_B2(Z, Y):\n",
    "    dB = (Z-Y)*Y*(1-Y)\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z-Y)*Y*(1-Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous lab, we are not making use of the sigmoid function inside the update rules. Instead, we feed them the outputs from the middle layer (H, in this example). The results are the same and which expression you use is simply a matter of readbility and compactness of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Accuracy:  0.5989700511155638\n",
      "Epoch:  50  Accuracy:  0.8783908508123914\n",
      "Epoch:  100  Accuracy:  0.9767655564322406\n",
      "Epoch:  150  Accuracy:  0.9925573656891489\n",
      "Epoch:  200  Accuracy:  0.9963154570874925\n",
      "Epoch:  250  Accuracy:  0.9974946550842637\n",
      "Epoch:  300  Accuracy:  0.9979269264922713\n",
      "Epoch:  350  Accuracy:  0.9981018779849045\n",
      "Epoch:  400  Accuracy:  0.9981784547771303\n",
      "Epoch:  450  Accuracy:  0.998214871296848\n",
      "Epoch:  500  Accuracy:  0.9982341604578601\n",
      "Epoch:  550  Accuracy:  0.9982459075499903\n",
      "Epoch:  600  Accuracy:  0.9982542377598749\n",
      "Epoch:  650  Accuracy:  0.9982609672670295\n",
      "Epoch:  700  Accuracy:  0.9982669085997296\n",
      "Epoch:  750  Accuracy:  0.998272429853993\n",
      "Epoch:  800  Accuracy:  0.9982776998324931\n",
      "Epoch:  850  Accuracy:  0.9982827971953168\n",
      "Epoch:  900  Accuracy:  0.9982877595758042\n",
      "Epoch:  950  Accuracy:  0.9982926058383815\n",
      "Epoch:  1000  Accuracy:  0.9982973462125413\n",
      "Epoch:  1050  Accuracy:  0.9983019869212386\n",
      "Epoch:  1100  Accuracy:  0.9983065322993906\n",
      "Epoch:  1150  Accuracy:  0.9983109857653666\n",
      "Epoch:  1200  Accuracy:  0.9983153502675682\n",
      "Epoch:  1250  Accuracy:  0.9983196284906272\n",
      "Epoch:  1300  Accuracy:  0.9983238229515002\n",
      "Epoch:  1350  Accuracy:  0.9983279360451345\n",
      "Epoch:  1400  Accuracy:  0.9983319700670292\n",
      "Epoch:  1450  Accuracy:  0.9983359272251989\n",
      "Epoch:  1500  Accuracy:  0.9983398096472532\n",
      "Epoch:  1550  Accuracy:  0.9983436193851988\n",
      "Epoch:  1600  Accuracy:  0.9983473584191528\n",
      "Epoch:  1650  Accuracy:  0.9983510286605031\n",
      "Epoch:  1700  Accuracy:  0.998354631954764\n",
      "Epoch:  1750  Accuracy:  0.9983581700842347\n",
      "Epoch:  1800  Accuracy:  0.9983616447705141\n",
      "Epoch:  1850  Accuracy:  0.9983650576768973\n",
      "Epoch:  1900  Accuracy:  0.9983684104106634\n",
      "Epoch:  1950  Accuracy:  0.9983717045252647\n",
      "Epoch:  2000  Accuracy:  0.9983749415224232\n",
      "Epoch:  2050  Accuracy:  0.9983781228541363\n",
      "Epoch:  2100  Accuracy:  0.9983812499245979\n",
      "Epoch:  2150  Accuracy:  0.9983843240920371\n",
      "Epoch:  2200  Accuracy:  0.9983873466704806\n",
      "Epoch:  2250  Accuracy:  0.9983903189314378\n",
      "Epoch:  2300  Accuracy:  0.9983932421055179\n",
      "Epoch:  2350  Accuracy:  0.998396117383976\n",
      "Epoch:  2400  Accuracy:  0.9983989459201963\n",
      "Epoch:  2450  Accuracy:  0.998401728831112\n",
      "Epoch:  2500  Accuracy:  0.998404467198567\n",
      "Epoch:  2550  Accuracy:  0.9984071620706197\n",
      "Epoch:  2600  Accuracy:  0.9984098144627944\n",
      "Epoch:  2650  Accuracy:  0.9984124253592793\n",
      "Epoch:  2700  Accuracy:  0.9984149957140767\n",
      "Epoch:  2750  Accuracy:  0.998417526452105\n",
      "Epoch:  2800  Accuracy:  0.9984200184702567\n",
      "Epoch:  2850  Accuracy:  0.9984224726384121\n",
      "Epoch:  2900  Accuracy:  0.9984248898004129\n",
      "Epoch:  2950  Accuracy:  0.9984272707749965\n",
      "Epoch:  3000  Accuracy:  0.9984296163566916\n",
      "Epoch:  3050  Accuracy:  0.9984319273166793\n",
      "Epoch:  3100  Accuracy:  0.998434204403619\n",
      "Epoch:  3150  Accuracy:  0.998436448344441\n",
      "Epoch:  3200  Accuracy:  0.9984386598451092\n",
      "Epoch:  3250  Accuracy:  0.9984408395913517\n",
      "Epoch:  3300  Accuracy:  0.9984429882493643\n",
      "Epoch:  3350  Accuracy:  0.9984451064664855\n",
      "Epoch:  3400  Accuracy:  0.9984471948718452\n",
      "Epoch:  3450  Accuracy:  0.9984492540769883\n",
      "Epoch:  3500  Accuracy:  0.9984512846764744\n",
      "Epoch:  3550  Accuracy:  0.9984532872484537\n",
      "Epoch:  3600  Accuracy:  0.9984552623552212\n",
      "Epoch:  3650  Accuracy:  0.9984572105437491\n",
      "Epoch:  3700  Accuracy:  0.9984591323461999\n",
      "Epoch:  3750  Accuracy:  0.9984610282804186\n",
      "Epoch:  3800  Accuracy:  0.9984628988504073\n",
      "Epoch:  3850  Accuracy:  0.9984647445467816\n",
      "Epoch:  3900  Accuracy:  0.9984665658472088\n",
      "Epoch:  3950  Accuracy:  0.9984683632168321\n",
      "Epoch:  4000  Accuracy:  0.9984701371086758\n",
      "Epoch:  4050  Accuracy:  0.9984718879640381\n",
      "Epoch:  4100  Accuracy:  0.9984736162128675\n",
      "Epoch:  4150  Accuracy:  0.9984753222741265\n",
      "Epoch:  4200  Accuracy:  0.9984770065561417\n",
      "Epoch:  4250  Accuracy:  0.9984786694569399\n",
      "Epoch:  4300  Accuracy:  0.9984803113645736\n",
      "Epoch:  4350  Accuracy:  0.9984819326574337\n",
      "Epoch:  4400  Accuracy:  0.9984835337045508\n",
      "Epoch:  4450  Accuracy:  0.9984851148658862\n",
      "Epoch:  4500  Accuracy:  0.9984866764926119\n",
      "Epoch:  4550  Accuracy:  0.9984882189273809\n",
      "Epoch:  4600  Accuracy:  0.9984897425045877\n",
      "Epoch:  4650  Accuracy:  0.9984912475506192\n",
      "Epoch:  4700  Accuracy:  0.9984927343840978\n",
      "Epoch:  4750  Accuracy:  0.9984942033161147\n",
      "Epoch:  4800  Accuracy:  0.9984956546504553\n",
      "Epoch:  4850  Accuracy:  0.9984970886838177\n",
      "Epoch:  4900  Accuracy:  0.9984985057060219\n",
      "Epoch:  4950  Accuracy:  0.9984999060002133\n",
      "Epoch:  5000  Accuracy:  0.9985012898430583\n",
      "Epoch:  5050  Accuracy:  0.998502657504933\n",
      "Epoch:  5100  Accuracy:  0.9985040092501065\n",
      "Epoch:  5150  Accuracy:  0.9985053453369166\n",
      "Epoch:  5200  Accuracy:  0.9985066660179405\n",
      "Epoch:  5250  Accuracy:  0.9985079715401592\n",
      "Epoch:  5300  Accuracy:  0.9985092621451166\n",
      "Epoch:  5350  Accuracy:  0.9985105380690731\n",
      "Epoch:  5400  Accuracy:  0.9985117995431543\n",
      "Epoch:  5450  Accuracy:  0.9985130467934943\n",
      "Epoch:  5500  Accuracy:  0.9985142800413745\n",
      "Epoch:  5550  Accuracy:  0.9985154995033586\n",
      "Epoch:  5600  Accuracy:  0.9985167053914209\n",
      "Epoch:  5650  Accuracy:  0.9985178979130733\n",
      "Epoch:  5700  Accuracy:  0.9985190772714861\n",
      "Epoch:  5750  Accuracy:  0.9985202436656053\n",
      "Epoch:  5800  Accuracy:  0.9985213972902668\n",
      "Epoch:  5850  Accuracy:  0.9985225383363059\n",
      "Epoch:  5900  Accuracy:  0.9985236669906644\n",
      "Epoch:  5950  Accuracy:  0.9985247834364931\n",
      "Epoch:  6000  Accuracy:  0.998525887853252\n",
      "Epoch:  6050  Accuracy:  0.9985269804168068\n",
      "Epoch:  6100  Accuracy:  0.9985280612995224\n",
      "Epoch:  6150  Accuracy:  0.998529130670354\n",
      "Epoch:  6200  Accuracy:  0.9985301886949342\n",
      "Epoch:  6250  Accuracy:  0.9985312355356586\n",
      "Epoch:  6300  Accuracy:  0.9985322713517685\n",
      "Epoch:  6350  Accuracy:  0.9985332962994298\n",
      "Epoch:  6400  Accuracy:  0.9985343105318115\n",
      "Epoch:  6450  Accuracy:  0.9985353141991605\n",
      "Epoch:  6500  Accuracy:  0.9985363074488739\n",
      "Epoch:  6550  Accuracy:  0.99853729042557\n",
      "Epoch:  6600  Accuracy:  0.998538263271157\n",
      "Epoch:  6650  Accuracy:  0.9985392261248988\n",
      "Epoch:  6700  Accuracy:  0.9985401791234797\n",
      "Epoch:  6750  Accuracy:  0.998541122401067\n",
      "Epoch:  6800  Accuracy:  0.9985420560893714\n",
      "Epoch:  6850  Accuracy:  0.9985429803177062\n",
      "Epoch:  6900  Accuracy:  0.998543895213044\n",
      "Epoch:  6950  Accuracy:  0.9985448009000724\n",
      "Epoch:  7000  Accuracy:  0.9985456975012477\n",
      "Epoch:  7050  Accuracy:  0.9985465851368474\n",
      "Epoch:  7100  Accuracy:  0.9985474639250206\n",
      "Epoch:  7150  Accuracy:  0.9985483339818378\n",
      "Epoch:  7200  Accuracy:  0.9985491954213382\n",
      "Epoch:  7250  Accuracy:  0.9985500483555769\n",
      "Epoch:  7300  Accuracy:  0.9985508928946694\n",
      "Epoch:  7350  Accuracy:  0.9985517291468361\n",
      "Epoch:  7400  Accuracy:  0.9985525572184448\n",
      "Epoch:  7450  Accuracy:  0.9985533772140525\n",
      "Epoch:  7500  Accuracy:  0.9985541892364449\n",
      "Epoch:  7550  Accuracy:  0.9985549933866769\n",
      "Epoch:  7600  Accuracy:  0.9985557897641096\n",
      "Epoch:  7650  Accuracy:  0.9985565784664484\n",
      "Epoch:  7700  Accuracy:  0.9985573595897783\n",
      "Epoch:  7750  Accuracy:  0.9985581332285997\n",
      "Epoch:  7800  Accuracy:  0.9985588994758617\n",
      "Epoch:  7850  Accuracy:  0.9985596584229968\n",
      "Epoch:  7900  Accuracy:  0.9985604101599516\n",
      "Epoch:  7950  Accuracy:  0.9985611547752193\n",
      "Epoch:  8000  Accuracy:  0.9985618923558706\n",
      "Epoch:  8050  Accuracy:  0.9985626229875825\n",
      "Epoch:  8100  Accuracy:  0.9985633467546686\n",
      "Epoch:  8150  Accuracy:  0.9985640637401065\n",
      "Epoch:  8200  Accuracy:  0.998564774025566\n",
      "Epoch:  8250  Accuracy:  0.9985654776914354\n",
      "Epoch:  8300  Accuracy:  0.9985661748168483\n",
      "Epoch:  8350  Accuracy:  0.9985668654797084\n",
      "Epoch:  8400  Accuracy:  0.9985675497567154\n",
      "Epoch:  8450  Accuracy:  0.9985682277233878\n",
      "Epoch:  8500  Accuracy:  0.998568899454088\n",
      "Epoch:  8550  Accuracy:  0.9985695650220443\n",
      "Epoch:  8600  Accuracy:  0.9985702244993737\n",
      "Epoch:  8650  Accuracy:  0.9985708779571038\n",
      "Epoch:  8700  Accuracy:  0.9985715254651941\n",
      "Epoch:  8750  Accuracy:  0.998572167092557\n",
      "Epoch:  8800  Accuracy:  0.9985728029070778\n",
      "Epoch:  8850  Accuracy:  0.9985734329756346\n",
      "Epoch:  8900  Accuracy:  0.9985740573641179\n",
      "Epoch:  8950  Accuracy:  0.9985746761374491\n",
      "Epoch:  9000  Accuracy:  0.9985752893595992\n",
      "Epoch:  9050  Accuracy:  0.9985758970936066\n",
      "Epoch:  9100  Accuracy:  0.9985764994015947\n",
      "Epoch:  9150  Accuracy:  0.9985770963447891\n",
      "Epoch:  9200  Accuracy:  0.9985776879835341\n",
      "Epoch:  9250  Accuracy:  0.9985782743773094\n",
      "Epoch:  9300  Accuracy:  0.9985788555847457\n",
      "Epoch:  9350  Accuracy:  0.9985794316636405\n",
      "Epoch:  9400  Accuracy:  0.9985800026709731\n",
      "Epoch:  9450  Accuracy:  0.9985805686629199\n",
      "Epoch:  9500  Accuracy:  0.9985811296948682\n",
      "Epoch:  9550  Accuracy:  0.9985816858214314\n",
      "Epoch:  9600  Accuracy:  0.9985822370964617\n",
      "Epoch:  9650  Accuracy:  0.9985827835730647\n",
      "Epoch:  9700  Accuracy:  0.9985833253036122\n",
      "Epoch:  9750  Accuracy:  0.9985838623397549\n",
      "Epoch:  9800  Accuracy:  0.9985843947324359\n",
      "Epoch:  9850  Accuracy:  0.9985849225319019\n",
      "Epoch:  9900  Accuracy:  0.9985854457877166\n",
      "Epoch:  9950  Accuracy:  0.9985859645487715\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(10000):\n",
    "    Y, H = Forward(X, W1, B1, W2, B2)\n",
    "\n",
    "    W2 = W2 + learning_rate * diff_W2(H, Z, Y).T\n",
    "    B2 = B2 + learning_rate * diff_B2(Z, Y)\n",
    "    W1 = W1 + learning_rate * diff_W1(X, H, Z, Y, W2).T\n",
    "    B1 = B1 + learning_rate * diff_B1(Z, Y, W2, H)\n",
    "    if not epoch % 50:\n",
    "        Accuracy = 1 -np.mean((Z-Y)**2)\n",
    "        print(\"Epoch: \", epoch, \" Accuracy: \", Accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8944ac20941febd3898b0b9a08c0e7cafc7c4835a26d6ed6b5e365f07f1b2728"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
