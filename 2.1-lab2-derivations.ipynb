{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - derivations\n",
    "\n",
    "_\"Unlike the previous lab, we are not making use of the sigmoid function inside the update rules. Instead, we feed them the outputs from the middle layer (H, in this example). The results are the same and which expression you use is simply a matter of readbility and compactness of code.\"_\n",
    "\n",
    "Remember that the network looks as follows:\n",
    "\n",
    "<img src=\".\\i\\lab2.png\" width=\"400\"> </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_B2(Z, Y):\n",
    "    dB = (Z-Y)*Y*(1-Y)\n",
    "    return dB.sum(axis=0)\n",
    "\n",
    "def diff_W2(H, Z, Y):\n",
    "    dW = (Z-Y)*Y*(1-Y)\n",
    "    return H.T.dot(dW)\n",
    "\n",
    "def diff_W1(X, H, Z, Y, W2):\n",
    "    dZ = (Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def diff_B1(Z, Y, W2, H):\n",
    "    return ((Z-Y).dot(W2)*Y*(1-Y)*H*(1-H)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotating the network diagram in more detail and derivating the above functions:\n",
    "\n",
    "<img src=\".\\i\\derivation-lab2.png\" width=\"400\"> </br>\n",
    "\n",
    "Use a simplified network consisting of only one hidden layer and one neuron per layer to understand dy_bar/dH.\n",
    "\n",
    "<img src=\".\\i\\backprop-neuron-net.png\" width=\"400\"> </br>\n",
    "\n",
    "If you want to find the update term for each individual weight (eg w_11) you will need to calculate the partial derivatives (see notes from class)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
