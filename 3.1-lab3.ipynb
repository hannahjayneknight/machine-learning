{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - playing around\n",
    "\n",
    "Using the same code as in ```1.0-lab3.ipynb``` but with the following changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the sigmoid function with a ReLu function (F.relu instead of torch.sigmoid)\n",
    "_Shown in the code below_</br></br>\n",
    "\n",
    "This produces an accuracy of 0.141 which is higher than the sigmoid function but still very low. \n",
    "\n",
    "**The ReLu function**</br>\n",
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "<img src=\".\\i/ReLu.png\" width=\"400\"> </br>\n",
    "\n",
    "Source: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.096\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the optimizer from stochastic gradient descent to Adam (optim.Adam instead of optim.SGD)\n",
    "_Shown in the code below_</br></br>\n",
    "\n",
    "- With a sigmoid function: 0.941\n",
    "- With a ReLu function: 0.905\n",
    "\n",
    "**Why does the sigmoid function produce a better accuracy here?** Not dataset dependent. Choice of algorithm guided by problem that you have. You get better at this by LOTS of practice.\n",
    "\n",
    "**What is Adam gradient descent?** This avoids all the problems with gradient descent such as choosing the learning rate, functions being non-differentiable etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.114\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and modify the number of layers and the number of neurons in the hidden layer (be careful that the numbers are consistent between contiguous layers)\n",
    "\n",
    "1. 2 layers\n",
    "\n",
    "Sigmoid, Adam gradient descent: 0.953 \n",
    "_Does this high accuracy suggest that the third and fourth layers are unnecessary? Could suggest that these layers are leading to overfitting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.953\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 10 layers\n",
    "\n",
    "Sigmoid, Adam gradient descent: 0.114 </br>\n",
    "ReLu, Adam gradient descent: 0.29 </br></br>\n",
    "\n",
    "Sigmoid, SGD gradient descent: 0.103</br>\n",
    "ReLu, SGD gradient descent: 0.096</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.29\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.fc7 = nn.Linear(64, 64)\n",
    "        self.fc8 = nn.Linear(64, 64)\n",
    "        self.fc9 = nn.Linear(64, 64)\n",
    "        self.fc10 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = F.relu(self.fc9(x))\n",
    "        x = self.fc10(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct =0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net.forward(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. One hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing information about the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0050, -0.0250, -0.0321,  ..., -0.0256,  0.0284,  0.0227],\n",
       "        [-0.0047, -0.0292, -0.0025,  ..., -0.0169,  0.0189,  0.0198],\n",
       "        [ 0.0049,  0.0290,  0.0123,  ..., -0.0263, -0.0150,  0.0029],\n",
       "        ...,\n",
       "        [-0.0315, -0.0136, -0.0255,  ...,  0.0055, -0.0257,  0.0316],\n",
       "        [ 0.0269, -0.0329,  0.0006,  ..., -0.0051, -0.0151, -0.0046],\n",
       "        [ 0.0092,  0.0347, -0.0118,  ..., -0.0276, -0.0095, -0.0245]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0117,  0.0101, -0.0080,  0.0214, -0.0239, -0.0063, -0.0026, -0.0259,\n",
       "         0.0222,  0.0244, -0.0140, -0.0179,  0.0065,  0.0297,  0.0209, -0.0109,\n",
       "        -0.0024,  0.0180, -0.0183,  0.0140,  0.0172,  0.0290,  0.0352,  0.0226,\n",
       "         0.0054, -0.0292, -0.0288, -0.0087, -0.0055, -0.0276,  0.0151,  0.0268,\n",
       "         0.0008,  0.0322,  0.0252, -0.0049,  0.0039,  0.0320,  0.0124, -0.0052,\n",
       "        -0.0004, -0.0345, -0.0249,  0.0339, -0.0193, -0.0200, -0.0204,  0.0137,\n",
       "        -0.0191, -0.0247,  0.0320,  0.0250,  0.0280, -0.0126,  0.0002,  0.0195,\n",
       "         0.0039,  0.0265, -0.0247,  0.0180,  0.0019, -0.0192,  0.0134, -0.0061],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([10, 1, 28, 28])\n",
      "Shape of y: torch.Size([10]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in testset:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc7): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc8): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc9): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc10): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of hidden layers and nodes in a feedforward neural network\n",
    "\n",
    "Every NN has three types of layers: input, hidden, and output.\n",
    "\n",
    "Creating the NN architecture therefore means coming up with values for the number of layers of each type and the number of nodes in each of these layers.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "**The Input Layer**\n",
    "\n",
    "Simple--every NN has exactly one of them--no exceptions that I'm aware of.\n",
    "\n",
    "With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "**The Output Layer**\n",
    "\n",
    "Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.\n",
    "\n",
    "Is your NN going running in Machine Mode or Regression Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing). Machine mode: returns a class label (e.g., \"Premium Account\"/\"Basic Account\"). Regression Mode returns a value (e.g., price).\n",
    "\n",
    "If the NN is a regressor, then the output layer has a single node.\n",
    "\n",
    "If the NN is a classifier, then it also has a single node unless softmax is used in which case the output layer has one node per class label in your model.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "**The Hidden Layers**\n",
    "\n",
    "So those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leaves the hidden layers.\n",
    "\n",
    "How many hidden layers? Well if your data is linearly separable (which you often know by the time you begin coding a NN) then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job.\n",
    "\n",
    "Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ for an excellent summary of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.\n",
    "\n",
    "So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, author of Introduction to Neural Networks in Java offers a few more.\n",
    "\n",
    "In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "**_Pruning_** describes a set of techniques to trim network size (by nodes not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on a network architecture, err on the side of more neurons, if you add a pruning step.\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "**_Overfitting_** can be prevented by keeping the number of neurons below:\n",
    "\n",
    "Nh=Ns(α∗(Ni+No))\n",
    "\n",
    "Ni = number of input neurons.\n",
    "No = number of output neurons.\n",
    "Ns = number of samples in training data set.\n",
    "α = an arbitrary scaling factor usually 2-10.\n",
    "\n",
    "<img src=\".\\i\\neuron-number.png\" width=\"400\"> </br>\n",
    "\n",
    "Others recommend setting α to a value between 5 and 10, but I find a value of 2 will often work without overfitting. You can think of α as the effective branching factor or number of nonzero weights for each neuron. Dropout layers will bring the \"effective\" branching factor way down from the actual mean branching factor for your network.\n",
    "\n",
    "As explained by this excellent NN Design text, you want to limit the number of free parameters in your model (its degree or number of nonzero weights) to a small portion of the degrees of freedom in your data. The degrees of freedom in your data is the number samples * degrees of freedom (dimensions) in each sample or Ns∗(Ni+No) (assuming they're all independent). So α is a way to indicate how general you want your model to be, or how much you want to prevent overfitting.\n",
    "\n",
    "For an automated procedure you'd start with an α of 2 (twice as many degrees of freedom in your training data as your model) and work your way up to 10 if the error (loss) for your training dataset is significantly smaller than for your test dataset.\n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ec054b59268ae76760b157b3eba00de255c2d102e132ab4e8bc4a94d89a299"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
